# 4. Deploying and Configuring Airflow

In this step, we will install and configure **Apache Airflow** on the **GCP VM** under the `airflow` user. This includes creating its environment, configuring necessary services, and testing the installation.

## Step 1: Switch to the Airflow User

### Log in to the VM as the Airflow user:
```bash
ssh airflow@<vm-ip>
```
**Example:**
```bash
ssh airflow-vm
```

### Switch from your current user to the Airflow user:
```bash
sudo su - airflow
```
**Why?**
This ensures that all Airflow-specific files, configurations, and environments are managed by the `airflow` user.

---

## Step 2: Set Up Airflow Environment

### Install Python and Virtual Environment Tools:
```bash
sudo apt update && sudo apt install -y python3 python3-pip python3-venv
```
**Why?**
- Airflow requires **Python 3** and `pip` to run.
- A **virtual environment** isolates the Airflow installation from the system environment.

### Create and Activate a Virtual Environment:
```bash
python3 -m venv airflow-venv
source airflow-venv/bin/activate
```
**Why?**
- Using a **virtual environment** prevents dependency conflicts with other Python applications on the system.

### Upgrade `pip` and Install Airflow:
```bash
pip install --upgrade pip
pip install apache-airflow
```
**Why?**
- Installing Airflow in the **virtual environment** ensures that all required dependencies are installed in an isolated manner.

---

## Step 3: Configure Airflow

### Set the Airflow Home Directory:
Define `AIRFLOW_HOME` to specify where Airflow files and logs will be stored:
```bash
export AIRFLOW_HOME=~/airflow
mkdir -p $AIRFLOW_HOME
```

#### Add this to your shell configuration (`~/.bashrc`) for persistence:
```bash
echo "export AIRFLOW_HOME=~/airflow" >> ~/.bashrc
source ~/.bashrc
```
**Why?**
- This centralizes all Airflow-related files in a dedicated directory for better organization and easier troubleshooting.

### Initialize the Airflow Database:
```bash
airflow db init
```
**What It Does:**
- Creates the **metadata database** where Airflow stores information about **DAGs, tasks, and their states**.

### Create an Admin User:
```bash
airflow users create \
  --username admin \
  --firstname Admin \
  --lastname User \
  --role Admin \
  --email admin@example.com
```
**Why?**
- This user will be used to log in to the **Airflow web interface**.

---

## Step 4: Start Airflow Services

### Start the Webserver:
```bash
airflow webserver -p 8080
```
**What It Does:**
- Starts the **Airflow web interface**, accessible at `http://<vm-ip>:8080`.

### Start the Scheduler:
Open a new terminal window (or use `tmux/screen` to run both processes in parallel):
```bash
airflow scheduler
```
**Why?**
- The **scheduler** is responsible for running tasks according to the **DAG schedule**.

---

## Step 5: Configure Airflow for Airbyte Integration

### Install Additional Airflow Providers:
For **API-based integration**:
```bash
pip install apache-airflow-providers-http
```
**Why?**
- The `http` provider allows you to **make API calls from Airflow**, which will be used to trigger Airbyte jobs.

### Create a DAG for Airbyte:
Navigate to the `dags/` directory:
```bash
mkdir -p $AIRFLOW_HOME/dags
```

#### Create a new DAG file (`airbyte_integration.py`):
```bash
nano $AIRFLOW_HOME/dags/airbyte_integration.py
```

#### Example DAG:
```python
from airflow import DAG
from airflow.providers.http.operators.http import SimpleHttpOperator
from datetime import datetime

with DAG(
    "airbyte_integration",
    default_args={"retries": 1},
    schedule_interval=None,
    start_date=datetime(2023, 1, 1),
    catchup=False,
) as dag:
    trigger_airbyte = SimpleHttpOperator(
        task_id="trigger_airbyte_sync",
        http_conn_id="airbyte_api",
        endpoint="api/v1/jobs/sync",
        method="POST",
        headers={"Content-Type": "application/json"},
        data='{"connectionId": "your-airbyte-connection-id"}',
    )
```
**Why?**
- This DAG **triggers Airbyte sync jobs** via its API and integrates them into your **larger workflows**.

### Configure Airflow HTTP Connection:
In the **Airflow web UI** (`http://<vm-ip>:8080`), navigate to **Admin → Connections** and add an **HTTP connection**:
- **Conn ID:** `airbyte_api`
- **Host:** `http://localhost:8000` (Airbyte’s API endpoint)
- **Extra:** `{"Authorization": "Bearer your-auth-token"}` *(if authentication is required)*

**Why?**
- This sets up the **connection Airflow uses to communicate with Airbyte**.

---

## Step 6: Test the Setup

### Access the Airflow Web Interface:
Open `http://<vm-ip>:8080` in your browser and log in using the **admin credentials**.

### Trigger the Airbyte DAG:
In the **Airflow web interface**, locate the `airbyte_integration` DAG and **trigger it manually**.

### Verify Logs:
Check the **Airflow task logs** to ensure the **API call was successful** and the **Airbyte job was triggered**.

---

## Best Practices for Airflow Setup

### Resource Allocation:
- **Monitor resource usage** on the VM (**CPU, memory**) and **adjust the instance size if necessary**.

### Logging:
- Ensure **Airflow logs are stored in a persistent location** (`$AIRFLOW_HOME/logs`) for **troubleshooting**.

### Security:
- Use Airflow’s **built-in RBAC system** to control access to the web interface.
- Never expose Airflow’s **API or webserver** to public networks **without proper authentication**.

---

The next step will cover **deploying and configuring Airbyte** on the VM, followed by **integration between the two services**.