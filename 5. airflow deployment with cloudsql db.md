# Production-Grade Deployment of Apache Airflow

A production-grade **Apache Airflow** deployment requires considerations for **scalability, security, and reliability**. Below is a comprehensive guide to deploying Airflow in a production environment on a **GCP VM**, focusing on best practices and robust configurations.

---

## 1. Architectural Overview
For a production setup, Airflow should:
- Run on a **dedicated GCP VM** with sufficient resources.
- Use a **PostgreSQL** database as the metadata database (instead of SQLite, which is for testing only).
- Leverage a **CeleryExecutor** for distributed task execution.
- Store logs in **Google Cloud Storage (GCS)** or another external storage for persistence.
- Expose the webserver securely with **RBAC authentication**.

---

## 2. Prerequisites

### VM Specifications:
- **Machine type**: `e2-standard-4` (4 vCPUs, 16 GB RAM) or higher.
- **Disk**: At least **50GB** to handle logs and temporary data.
- **OS**: Debian 11 or Ubuntu 20.04 LTS.

### Software and Tools:
- **Python 3.7 or higher**.
- **PostgreSQL database** (managed Cloud SQL or self-hosted on another GCP VM).
- **Docker and Docker Compose** (optional, but recommended for easier deployment).

---

## 3. Deployment Steps

### Step 1: Create the GCP VM
1. Go to **GCP Console → Compute Engine → VM Instances**.
2. Configure the VM:
   - **Name**: `airflow-production`
   - **Machine Type**: `e2-standard-4 (4 vCPUs, 16 GB RAM)`
   - **Boot Disk**: Ubuntu 20.04 LTS, 50GB
   - **Firewall**: Allow HTTP and HTTPS traffic.
3. **SSH into the VM**:
```bash
gcloud compute ssh airflow-production --zone=us-central1-a
```

---

### Step 2: Install System Dependencies
```bash
sudo apt update && sudo apt upgrade -y
sudo apt install -y python3 python3-pip python3-venv libpq-dev postgresql-client rabbitmq-server nginx
```

---

### Step 3: Set Up the Metadata Database (PostgreSQL)
Use **Google Cloud SQL** for the metadata database:
```bash
gcloud sql instances create airflow-db --database-version=POSTGRES_14 --tier=db-f1-micro --region=northamerica-northeast2
```
```bash
gcloud sql databases create airflow_metadata --instance=airflow-db
gcloud sql users set-password postgres --host=% --instance=airflow-db --password=StrongPass@123
```
If not able to create GCP database then visit this: <I will add the linked md here>

![Alt text](images/gcloud_sql_auth_error_redacted.png)

Retrieve the connection string for PostgreSQL:
```plaintext
postgresql+psycopg2://postgres:StrongPass@123@<CLOUD_SQL_IP>:5432/airflow_metadata
```
Test connectivity:
```bash
psql -h <CLOUD_SQL_IP> -U postgres -d airflow_metadata
```

---

### Step 4: Install Airflow

#### Create a dedicated `airflow` user:
```bash
sudo adduser airflow
sudo su - airflow
```

#### Set up a Python virtual environment:
```bash
python3 -m venv airflow-venv
source airflow-venv/bin/activate
```

#### Install Airflow and dependencies:
```bash
export AIRFLOW_VERSION=2.10.4
pip install "apache-airflow==${AIRFLOW_VERSION}" "apache-airflow[celery,postgres]==${AIRFLOW_VERSION}"
```
[Apache Airflow Release Notes](https://airflow.apache.org/docs/apache-airflow/stable/release_notes.html)
 — Airflow Documentation
Airflow release version 

---

### Step 5: Configure Airflow

#### Define the Airflow home directory:
```bash
export AIRFLOW_HOME=~/airflow
mkdir -p $AIRFLOW_HOME
echo "export AIRFLOW_HOME=~/airflow" >> ~/.bashrc
source ~/.bashrc
```

#### Initialize the database:
```bash
airflow db init
```

#### Update `airflow.cfg`:
```bash
nano $AIRFLOW_HOME/airflow.cfg
```
Set **executor** to **CeleryExecutor**:
```ini
executor = CeleryExecutor
```
Set the **database connection string**:
```ini
sql_alchemy_conn = postgresql+psycopg2://postgres:StrongPass@123@<CLOUD_SQL_IP>:5432/airflow_metadata
```
Configure **remote logging**:
```ini
remote_logging = True
remote_log_conn_id = gcs_default
remote_base_log_folder = gs://airflow-logs
```

---

### Step 6: Set Up Celery

#### Configure RabbitMQ as the broker:
```bash
sudo systemctl enable rabbitmq-server
sudo systemctl start rabbitmq-server
sudo systemctl status rabbitmq-server
```

Update **airflow.cfg**:
```ini
broker_url = pyamqp://guest:guest@localhost:5672/
result_backend = db+postgresql://postgres:StrongPass@123@<CLOUD_SQL_IP>:5432/airflow_metadata
```

---

### Step 7: Secure Airflow

#### Create an admin user:
```bash
airflow users create \
  --username admin \
  --firstname Admin \
  --lastname User \
  --role Admin \
  --email admin@example.com
```

#### Secure the Webserver with HTTPS:

1. Install **SSL certificates** using Let’s Encrypt or self-signed certs.
2. Set up **NGINX** as a reverse proxy:
```bash
sudo nano /etc/nginx/sites-available/airflow
```
```nginx
server {
    listen 443 ssl;
    server_name airflow.yourdomain.com;
    ssl_certificate /etc/letsencrypt/live/airflow.yourdomain.com/fullchain.pem;
    ssl_certificate_key /etc/letsencrypt/live/airflow.yourdomain.com/privkey.pem;
    location / {
        proxy_pass http://localhost:8080;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    }
}
```
Restart **NGINX**:
```bash
sudo ln -s /etc/nginx/sites-available/airflow /etc/nginx/sites-enabled/
sudo systemctl restart nginx
```

---

### Step 8: Start Airflow Services
```bash
airflow webserver -p 8080
```
```bash
airflow scheduler
```
```bash
airflow celery worker
```

---

### Step 9: Test and Validate
- Access the **Airflow web UI**: `https://airflow.yourdomain.com`
- Verify that tasks are executed using the **Celery worker**.

---

### Step 10: Monitor and Scale
Enable **Google Cloud Monitoring**:
```bash
gcloud monitoring dashboards create
```
Scale by **adding more worker VMs**:
- Connect them to RabbitMQ and the metadata database.

---

## Best Practices
- **Database**: Use a **managed Cloud SQL instance** for high availability.
- **Logging**: Store logs in **GCS** for durability.
- **Security**: Protect web server access using **RBAC and HTTPS**.
- **Backup**: Regularly **back up the metadata database**.

This setup ensures Airflow is **secure, scalable, and production-ready**.
